# MLP



### Linear Neural Networks

optimization variables

: parameter가 어디로 움직여야 loss function이 작아지는지 찾는 것이 관건!

-> gradient descent

: W만 곱해나가면 그냥 하나의 W를 곱하는 것과 다를 것이 없음

-> 그래서 중간에 들어가 주는 것이 activation function



activation functions

: ReLU, Sigmoid, Hyperbolic Tangent -> 문제나 상황마다 다름



