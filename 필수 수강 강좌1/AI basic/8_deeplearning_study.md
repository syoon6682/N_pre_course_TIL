# Deep learning study

: 신경망 - 비선형모델



### 선형모델 수식 분해

O: 행벡터

X: 데이터(출력벡터)

W: 가중치 행렬

b: 합해주는 행렬

O = XW + b



### 소프트맥스 함수(softmax)

: 모델의 출력을 확률로 해석할 수 있게 변환해주는 연산

: 학습이 아닌 추론의 경우 one_hot 벡터 활용 -> 최대값을 가진 주소만 1로 출력하는 연산

: 활성함수 = 하나의 실수값을 가지고 활성, 비활성을 판단하는 함수 (ex: 시그모이드, ReLU, tanh...)

-> 신경망은 선형모델과 활성함수를 합성한 함수!

: 선형모델로 나온 출력물을 활성함수로 한번 거친 것을 "퍼셉트론"이라고 부름

: 활성결과를 또 신경망으로 반복적으로 연결, 연결, 연결......이것이 신경망!  -> 다층 퍼센트론(multi-layer perceptron, MLP)

: 이러한 방법을 **순전파(forward propagation)**이라고 함 -> 학습이 아닌, 입력이 들어왔을때 출력이 나오는 과정

솔직히 이해되는듯, 근데 연산량이 진짜 많겠다..



#### 왜 층을 여러개 쌓나요

: 이론적으로는 2층 신경망으로도 임의의 연속함수를 근사할 수 있음

: 그러나 **층이 깊어질수록 목적함수를 근사하는데 필요한 뉴런(노드)의 숫자가 훨씬 빨리 줄어들어 좀더 효율적으로 학습이 가능**



### 역전파 알고리즘(backpropagation)

: 경사하강법을 활용한 각 층에 있는 파라미터에 대한 미분을 계산해서 파라미터를 업데이트함 (like 선형모델)

: 문제는 각 층별로 계산해야하므로 한번에 계산이 안됨 -> 역전파 알고리즘을 활용하여 계산(미분을 계산할떄 순차적으로 진행)

: 역전파 알고리즘 목적: 각각의 가중치 행렬 Wl에 대해서 손실함수에 대한 미분을 계산

: 손실함수 L 활용



#### 역전파 알고리즘 원리

: **합성함수 미분법**인 **연쇄법칙** 기반 자동미분 (연쇄법칙은 기억이 잘 안나네...교재 확인하고 기억 잘하기!)

: 순전파에 비해 많은 메모리를 차지함



